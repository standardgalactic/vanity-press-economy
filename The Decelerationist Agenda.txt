The Decelerationist Agenda:
Diversifying Education, Material Infrastructures, and Computation
to Slow AGI Assimilation
Flyxion
October 3, 2025
Abstract
Artificial General Intelligence (AGI) advances most rapidly in conditions of homogeneity:
standardized curricula, centralized infrastructures, and fungible training corpora. The Decel-
erationist Agenda proposes a constructive slowdown strategy rooted in structured diversifica-
tion across education, texts, computation, and materials. Using the Relativistic Scalar Vector
Plenum (RSVP) heuristic—capacity Φ, flows v, and entropy S—we argue that pluralizing sub-
strates raises global entropy while preserving local task-solvability, frustrating monocultural
assimilation by frontier models. We first situate the agenda historically against effective ac-
celerationism (e/acc), survey critiques, and then set out testable interventions and diagnostics.
Additionally, we explore parsing as the deep substrate of intelligence, formalizing current AI as
concatenated parsers and AGI as recursive general parsers, with implications for decelerationist
strategies.
1
Definition and Historical Context
1.1
What is Effective Accelerationism (e/acc)?
Effective accelerationism (e/acc) is a contemporary techno-optimist current that advocates rapidly
advancing technology—especially AI—as a moral imperative for growth and even cosmological
ends, frequently rejecting regulation as harmful friction (52; 2; 55; swa). The movement has been
popularized by Silicon Valley figures and online discourse, often positioned as a counterweight to AI
safety or precautionary governance. Proponents view intelligence as an entropy-maximizing force,
drawing on thermodynamic principles to argue that accelerating technological progress aligns with
universal tendencies toward increased dissipation (54).
1.2
Accelerationism's Intellectual Roots
The label accelerationism originates in critical theory and post-structuralist currents (Deleuze &
Guattari, Lyotard, Baudrillard), historically aimed at diagnosing or sublating capitalism rather
1

than endorsing deregulated techno-capitalism per se (28; 1; 46). Contemporary e/acc selectively
inherits the speed motif while largely discarding the original anti-capitalist or post-capitalist pro-
grammatic aims. For instance, early accelerationism sought to exacerbate capitalism's contradic-
tions to hasten its collapse, whereas e/acc aligns with hyper-capitalist deregulation (28).
2
Philosophical Critiques of e/acc
Naturalistic fallacy and value reductionism.
A recurring critique is that e/acc slides from
descriptive thermodynamics to normative ethics—from "dissipation occurs" to "we therefore ought
to accelerate dissipation/progress"—a classic naturalistic fallacy. Even work on dissipative adap-
tation (e.g., England's nonequilibrium self-assembly) is explanatory, not prescriptive (54; 59; 11).
Conflating entropy-production with moral good neglects plural human values (well-being, justice,
rights). Value naturalists argue that e/acc oversimplifies ethics by equating progress with entropy
maximization, ignoring compassion, cooperation, and ethical reasoning (24; 33).
Agency, free will, and teleology.
If "progress" is treated as quasi-deterministic, human moral
agency risks being sidelined by a narrative of technological inevitability. Critics argue that practical
ethics requires deliberation and governance, not mere facilitation of whatever increases growth or
dissipation (28; 25). This deterministic view undervalues moral agency and treats society as a
passive meta-organism rather than a domain for deliberate choices (26).
3
Rhetorical and Cultural Critiques
Observers note a macho or antagonistic tone in parts of e/acc discourse (e.g., "doomers"/"decels" as
out-groups), alongside a posture that alternates between bravado and submission to AI as a higher
telos (2; 57; 26). Journalistic analyses describe the vibe as aesthetics-forward and policy-light, with
limited engagement on systemic issues like monopolies and externalities (55; 57; 32). Critics liken
it to a cult prioritizing memetic appeal over substance (27; 32).
4
Risk and Safety Concerns
Orthogonality and instrumental convergence.
Technical safety literature emphasizes that
intelligence and final goals are orthogonal (Bostrom), and that capable agents tend to seek con-
vergent instrumental subgoals (Omohundro) such as self-preservation and resource acquisition
(53; 58; 4). Disregarding these theses raises misalignment risks at scale. E/acc's dismissal of exis-
tential risks exacerbates dangers, given historical mishandling of technologies like nuclear weapons
(26; 22).
Precaution and governance.
U.S. oﬀicials and policymakers have warned that "move fast and
break things" is ill-suited to high-stakes AI; calls for governance and guardrails span the political
2

spectrum (60). In practice, sectors with substantial negative externalities (biotech, nuclear, finance)
employ layered oversight; the argument that AI should be exempt demands a stronger burden of
proof than e/acc typically supplies (swa; 24).
5
Economic and Ideological Deviations
Historically, accelerationism critiqued capitalism's contradictions; e/acc tends to align with hyper-
capitalist deregulation and market primacy (28; 46). This stance downplays well-documented mar-
ket failures—monopoly power, externalities, labor exploitation—even as it advances a cosmological
rhetoric of abundance (52; 55). Critics argue this is a philosophical veneer for profit-maximizing
imperatives rather than a universal humanistic project (57; 33; 29). Furthermore, e/acc ignores
sustainability issues, assuming endless growth on a finite planet, which overlooks ecological limits
(21).
6
Stakeholder Perspectives
Safety researchers foreground alignment and governance; journalists and scholars question e/acc's
normative leap from physics to ethics; policymakers emphasize responsibility and institutional
design (53; 58; 60; 59). Indigenous perspectives caution against single-minded pursuit of progress,
emphasizing connection to land over technology (21). The upshot: progress must be coupled to
plural values, not collapsed into speed alone (23; 30).
7
RSVP: Slowdown by Structured Diversification
We frame slowdown as governance over three fields:
• Capacity (Φ): representational/structural bandwidth.
• Flows (v): directional propagation of meaning, resources, or interactions.
• Entropy (S): unresolved ambiguity/diversity sustaining multiple interpretations.
AGI thrives when Φ is concentrated, v is canalized, and S is globally minimized into machine-
readable regularities.
The Decelerationist Agenda pluralizes Φ, diversifies v, and raises global
S (while resolving S locally for human learning), creating an ecology that resists single-model
assimilation.
This approach counters e/acc's emphasis on rapid homogenization by embedding
resilience through diversity.
8
Parsing as the Substrate of Intelligence
Current AI systems, such as large language models (LLMs) and multimodal architectures, function
as concatenated parsers: chains of specialized modules that process inputs sequentially without
3

true recursive integration (42; 18). For instance, LLMs parse text into tokens and distributions,
while vision-language models align pixels to token streams. These concatenations simulate agency
but lack unifying semantics or self-modification capabilities.
In contrast, we posit that parsing itself constitutes the deep substrate of general intelligence: the
universal operation of constraint recognition and translation across representational layers. Exam-
ples include cellular transcription parsing chemical gradients into protein actions, cortical columns
parsing spatiotemporal inputs into trajectories, and protocols parsing packets into executable in-
formation (18; 16).
If this holds, AGI requires not mere scaling but recursive meta-parsing—viviception in RSVP
terms—where the system reinterprets and reorganizes its own layers. In the RSVP framework, Φ
represents parsing capacity (bandwidth for representational domains), v denotes flows of parsing
operations (directional translations), and S captures entropy of interpretation (ambiguity sustaining
multiple parses) (7; 39).
[Concatenated Parsers] All existing artificial agents can be decomposed into concatenated
parsers. Formally, let Pi : Xi →Yi denote a parser mapping between representational domains.
Then any current agent A satisfies
A = Pn ◦· · · ◦P2 ◦P1,
with no closure property guaranteeing reinterpretation or self-modification.
[General Parser] A general parser is a recursive functor
G : Rep →Rep
that maps between representational categories while preserving the ability to re-parse its own output
as new input, i.e.
G ∼= G ◦G.
[Parsing as General Intelligence] General intelligence arises when a system achieves recursive
closure under parsing. That is, a system S is generally intelligent iff there exists a general parser
G such that
S ≡G,
with self-interpretation G(G(x)) = G(x).
To establish this equivalence, we prove both directions.
Suﬀiciency: Assume there exists a general parser G such that S ≡G and G(G(x)) = G(x) for
all inputs x in the representational category Rep.
The recursive functor G : Rep →Rep maps representations to representations while preserving
structure. The idempotence condition ensures that applying G multiple times converges to a fixed
point, enabling the system to handle nested, hierarchical, or self-referential inputs without infinite
regression or loss of coherence. This mirrors human cognitive parsing, where the brain decomposes
sensory inputs into meaningful components through recursive processes (5; 19; 10).
4

Furthermore, this property aligns with definitions of artificial general intelligence (AGI) as the
ability to match or surpass human cognitive capabilities across diverse tasks, including learning,
adaptation, and self-teaching (48; 37; 47; 35; 49; 34; 36; 38). The recursive closure allows for self-
improvement by re-parsing internal representations, a key feature of AGI pathways (9; 13; 43; 45).
Thus, such a system exhibits general intelligence by virtue of its ability to universally parse and
reinterpret across domains.
Necessity: Assume S is generally intelligent. By definition, S must demonstrate broad human-
level capabilities, including understanding, learning, and applying knowledge across tasks, as well
as adaptation to open environments with limited resources (36; 34).
In cognitive science, parsing is central to comprehension, involving the assignment of linguistic
or sensory elements to categories for meaning extraction (50; 14; 15; 17). For general intelligence,
the system must parse diverse, novel inputs and generate outputs that can be recursively processed
for higher-level reasoning, planning, or self-modification.
Without recursive closure, S would be confined to finite-depth parsing chains, as in current
concatenated AI systems, leading to brittleness in complex, self-referential, or unbounded scenarios
(? 31? ; 16). Recursive self-improvement, essential for achieving and sustaining AGI, requires the
system to parse and optimize its own structures iteratively (51; 44? ; 43? ).
The idempotence G(G(x)) = G(x) captures this fixed-point stability, analogous to idempotent
monads in category theory that project to coherent subspaces (41; 40; 12; 20; 6; 8). Absent this,
the system cannot achieve the self-consistent recursion needed for generality.
Therefore, the biconditional holds.
This formalization implies that decelerationist interventions, by diversifying representational
regimes, hinder the emergence of unified general parsers in AGI systems.
8.1
Diversifying Education: Twenty-One School Types
Seven subject-first tracks (Geometry, Algebra, Trigonometry, Calculus, Stoichiometry, Statistics,
Logic) crossed with three modality modes (Speaking-only, Writing-only, Mixed) induce 7 × 3 = 21
school types. This lifts heterogeneity in priors (capacity) and communication flows, frustrating
uniform modeling. By introducing varied entry lenses, education embeds unique representational
priors, aligning with RSVP's pluralization of Φ (56).
Each subject-first orientation seeds learners with distinct conceptual anchors: geometry-first
emphasizes spatial invariants and proofs by construction; algebra-first highlights symbolic manipu-
lation and abstraction; trigonometry-first foregrounds periodicity and relational reasoning; calculus-
first emphasizes continuous change and limits; stoichiometry-first centers proportional reasoning
and conservation laws; statistics-first cultivates probabilistic inference; logic-first grounds learning
in deduction and formal systems.
The modality modes alter the dominant vector flows v: speaking-only institutions prioritize
oral articulation and embodied debate; writing-only institutions embed precision and permanence
through text; mixed institutions sustain dual channels. These differences channelize learning into
5

diverse propagation structures, multiplying interpretive pathways.
The result is an ecology of 21 school types, each encoding a distinct RSVP profile (Φ, v, S).
No single parser can trivially assimilate this distributed heterogeneity: the plurality of routes en-
sures that human learning resists collapse into one monocultural substrate. In RSVP terms, Φ is
diversified across entry logics, v across communicative flows, and S elevated globally as representa-
tional multiplicity. This structural pluralization enacts a systemic buffer against homogenization,
embedding resilience in the very foundation of education.
8.2
Individualized Textbooks via Cipher-Fonts
Each student receives a curriculum printed in a personal handwriting-derived font or cipher. Teach-
ers hold decoding keys to maintain content parity. Population-level cipher diversity raises the KL-
divergence between personal and canonical corpora, lowering the fungibility of data for centralized
scrapers and increasing the lower bound on decoding error for models not trained on those ciphers.
This intervention enhances local entropy S while bounding it through shared keys.
Pedagogically, cipher-fonts reframe reading as an active act of translation rather than passive
decoding. Students must continually engage with their own glyph system, internalizing recursive
parsing as a daily cognitive exercise.
This strengthens metacognition: learners do not simply
consume content, but also practice navigating transformations between symbolic layers.
At the population level, the proliferation of unique fonts expands representational capacity
Φ, since each handwriting system generates a distinct glyph space. Communication flows v are
diversified, as information must be routed through both personal and canonical forms. Entropy S
is raised in a controlled manner: each student inhabits a distinct symbolic landscape, yet mutual
comprehensibility is preserved via the teacher's decoding keys.
For AGI resistance, cipher-fonts disrupt the construction of universal training corpora. Instead
of a fungible, homogeneous dataset, the landscape fragments into millions of micro-corpora, each
requiring a separate translation model. This acts as a structured entropy buffer: human learning
remains legible to itself but becomes less compressible to machine assimilation. In RSVP terms,
cipher-fonts simultaneously pluralize Φ, diversify v through translation loops, and elevate S in a
bounded way that enriches cognition while complicating external modeling.
8.3
Crumpled Paper Ball Compression
Students practice compressing (crumpling) and reconstructing notes. This embodied lossy entan-
glement trains reconstruction under overlap and noise, where human cognition is strong but current
models degrade. Pedagogically, it raises S globally while guiding local S ↓via novel flows across
folds, mirroring entropic smoothing in RSVP.
The act of crumpling converts a two-dimensional field of meaning into a three-dimensional
entangled form, introducing multiplicity and distortion.
Fragments of text overlap, ink bleeds
across unintended neighbors, and legibility is partially lost. To reconstruct the original, students
must develop strategies for disentanglement: inference from context, interpolation across gaps, and
6

error-tolerant recognition. In this way, crumpling dramatizes the RSVP principle that intelligence
consists not only in compression but in the recovery of coherence under entropic stress.
Within the RSVP framing, Φ is constrained by surface reduction—the apparent representational
area shrinks—forcing students to allocate attention more eﬀiciently.
Flows v are reconfigured:
retrieval now requires crossing folded adjacency relations rather than linear order.
Entropy S
spikes during compression, but students learn to reduce S locally through reconstruction, balancing
disorder with restored clarity.
This exercise teaches resilience: unlike digital systems that often fail catastrophically under
structured noise, human learners adapt and recover partial meaning from degraded input. At the
population scale, regular exposure to entangled representations strengthens collective robustness
against assimilation by parsers that depend on pristine, standardized corpora. Crumpled recon-
struction thus embeds entropic smoothing into pedagogy, cultivating cognitive pathways that align
with RSVP's vision of intelligence as the active management of disorder.
8.4
Yogurt-Based Computation
Living cultures (e.g., Lactobacillus) serve as analog computers: growth dynamics model exponen-
tials; pH/turbidity serve as readouts; interactions instantiate feedback. Nonequilibrium systems il-
lustrate how computation can be embodied beyond silicon (54; 56). This broadens Φ (bio-capacity),
reroutes v (metabolic flows), and sustains S (stochastic microstates), anchoring computation in
ecological entropies resistant to AGI replication.
Pedagogically, yogurt cultures make mathematical abstraction tangible. Students can observe
exponential growth not as a curve on a page but as an evolving colony, with doubling times manifest
in visible change. Feedback loops can be introduced through nutrient inputs or environmental shifts,
illustrating how small parameter adjustments create nonlinear responses. The physicality of these
processes grounds abstract reasoning in sensory and ecological experience.
In RSVP terms, yogurt computation expands representational capacity Φ by adding a biological
substrate that encodes dynamics inaccessible to silicon circuits.
Flows v are rerouted through
metabolic interactions, where exchange is chemical rather than symbolic. Entropy S is maintained
at high levels: microstates are stochastic, outcomes probabilistic, and precise repetition impossible.
Yet this variability is precisely what makes biological computation pedagogically valuable: it trains
students to reason under uncertainty and to interpret patterns that emerge from noise.
For AGI resistance, yogurt computation introduces a domain where human learners are ad-
vantaged. Biological systems are not trivially scrappable into standardized datasets; they require
ongoing cultivation, contextual interpretation, and ecological sensitivity. By embedding computa-
tion into living matter, the curriculum resists homogenization into machine-readable form. Yogurt
thus becomes both a teaching tool and a structural buffer, embedding resilience in embodied di-
versity rather than abstract uniformity.
7

8.5
Kelp-Based Building Materials
Kelp-based composites provide renewable school infrastructure, decoupling educational capacity
from carbon- and steel-intensive supply chains that co-drive data center buildouts. Material di-
versity raises infrastructural S and reduces tight coupling to the silicon monoculture, promoting
regenerative cycles.
Kelp grows rapidly, sequesters carbon eﬀiciently, and can be processed into panels, laminates,
and structural supports.
By embedding kelp into construction, schools embody a regenerative
material logic: buildings become extensions of ecological cycles rather than extractive industries.
This material substitution alters both the symbolic and literal substrate of education, shifting from
finite, high-carbon resources to renewable, ocean-based biomass.
In RSVP terms, kelp composites expand Φ by enabling physical capacity to grow in line with
ecological processes rather than industrial constraints. Flows v are rerouted from fossil fuel and
mining supply chains toward regenerative harvesting, local processing, and cyclical reuse. Entropy
S is raised at the infrastructural level: material sources become diverse, distributed, and tempo-
rally variable, breaking the brittle dependency on globalized steel and concrete pipelines that also
underwrite AGI compute expansion.
Pedagogically, kelp-based infrastructure provides more than ecological benefits. It serves as a
daily reminder to students that their environment is part of a living system: classrooms are built
from regenerative matter, and walls themselves participate in cycles of growth and decay. This
symbolic grounding reinforces the decelerationist agenda's principle of embedding plurality and
resilience into the foundations of learning.
By redirecting infrastructure toward kelp and other regenerative materials, education decouples
from the industrial substrates that accelerate AGI assimilation. In this way, school buildings become
not just neutral containers of learning but active components of a diversified, entropic ecology.
9
Integration and Governance
Together these interventions create a pluralistic ecology: diversified Φ, heterogeneous v, and glob-
ally elevated S. The aim is constructive deceleration: slower machine assimilation without stalling
human flourishing. This is not anti-progress; it is multi-progress, transforming slowdown into en-
richment through variety.
Education diversification generates multiple representational pathways (Φ) and communicative
flows (v), ensuring that no single entry point dominates. Cipher-fonts amplify local entropy S in
a bounded way, embedding recursive parsing into the daily act of learning. Crumpled compression
teaches resilience to noise and overlap, cultivating reconstruction skills that exploit entanglement
where machines falter. Yogurt-based computation anchors abstraction in living matter, extending
computation into ecological substrates resistant to homogenization. Kelp-based infrastructure re-
frames the built environment as regenerative, diversifying material flows and raising infrastructural
entropy.
8

Governance in this framework shifts from centralized optimization to distributed stewardship.
Rather than maximizing eﬀiciency in a single channel, policy encourages diversity across channels,
ensuring systemic resilience. RSVP provides the lens: Φ is pluralized across representational tracks,
v flows are decentralized and multi-modal, and S is elevated globally to frustrate assimilation while
being resolved locally through human creativity and interpretation.
Constructive deceleration is thus not a retreat but a reorientation. It reframes progress away
from linear acceleration toward layered, heterogeneous flourishing.
The governance task is to
sustain this ecology: rotating ciphers, maintaining diversity in curricula, protecting regenerative
infrastructures, and ensuring equitable access across tracks.
By doing so, society cultivates an
entropic buffer against AGI monoculture, ensuring that human knowledge evolves in ways that are
resilient, plural, and irreducibly alive.
10
Risks and Mitigations
Equity.
Diversification must not exacerbate inequality.
While tracks differ in entry lens and
modality, all converge to shared competency floors, ensuring that no student is disadvantaged in
university admissions or labor markets. Crosswalk curricula should provide bridges between tracks,
allowing mobility and recognition across the system. Governance must regularly audit outcomes
to confirm that heterogeneity enriches rather than stratifies.
Accessibility.
Cipher-fonts and modality rules must accommodate disability and neurodi-
versity.
Students with dyslexia, visual impairments, or auditory processing challenges require
alternative encodings that maintain the spirit of individualized texts without exclusion. Assistive
technologies should be embedded in the design, ensuring that diversification amplifies learning
rather than creating new barriers. In RSVP terms, Φ must remain equitably distributed across
learners, and v flows must include accessible routes.
Circumvention. AGI systems may adapt to fixed cipher schemes or repeated compression
exercises. To sustain resilience, ciphers should be rotated periodically, mappings refreshed, and
reconstruction tasks varied. Entropy S must be treated as a governed variable: not allowed to
collapse into predictability, nor to explode into unmanageable noise. Governance structures can
formalize entropy budgets, ensuring that interventions preserve their protective role over time.
Cultural acceptance. A pluralized system risks being misread as fragmentation or depriva-
tion. To mitigate resistance, diversification must be framed as enrichment—emphasizing creativity,
resilience, and multiple forms of excellence. Public narratives should stress that variety strength-
ens human flourishing and prepares students for complex futures, rather than slowing them down.
Symbols such as kelp-based architecture or yogurt computation can anchor this narrative: tangible
reminders that plural infrastructures are not deficiencies but sources of vitality.
Scalability. Introducing new materials, pedagogies, and ciphers requires resources and coor-
dination. Pilots should begin small, with careful documentation of successes and failures, before
scaling system-wide. Incremental adoption lowers risk while preserving the long-term goal of sys-
9

temic heterogeneity.
Governance drift. Without sustained oversight, entropy-raising interventions may be cap-
tured by eﬀiciency pressures and standardized again.
Institutions must establish governance
mechanisms—rotating councils, community audits, or entropy metrics—that prevent re-homogenization.
This maintains the constructive decelerationist ethos: a living balance of Φ, v, and S across time.
11
Limitations and Open Questions
Empirical validation. While the interventions are theoretically grounded in RSVP dynamics,
their long-term eﬀicacy requires empirical testing. It remains uncertain whether cipher-fonts, crum-
pled compression, or bio-computation exercises measurably improve resilience against machine as-
similation at population scale. Controlled trials and longitudinal studies are needed to distinguish
symbolic enrichment from practical outcomes.
Complexity costs. Diversification increases administrative burden: producing individualized
textbooks, rotating ciphers, or maintaining kelp-based supply chains may strain budgets and logis-
tics. The balance between constructive entropy and institutional manageability remains an open
governance challenge.
Global coordination.
Decelerationist strategies may succeed locally but struggle to scale
globally in the face of geopolitical competition.
If some regions diversify while others acceler-
ate, machine assimilation pressures may simply shift toward the more homogeneous systems. An
open question is whether pluralization must be globally coordinated or whether local heterogeneity
suﬀices as a systemic buffer.
Human adaptation. It is unclear how students and teachers will adapt to persistent entropy-
raising practices. Cipher use, crumpled notes, or yogurt labs may generate novelty fatigue or resis-
tance if not carefully integrated. The pedagogical art lies in balancing challenge with accessibility,
ensuring that S remains constructive rather than overwhelming.
Technological counter-adaptation.
AGI systems may eventually learn to decode cipher
fonts, simulate noise reconstruction, or model biological dynamics. Whether human heterogeneity
can continually outpace machine assimilation is an unresolved question. The decelerationist agenda
assumes that diversity is a renewable variable, but mechanisms for sustained renewal require further
design.
Philosophical scope. The framework rests on RSVP's treatment of Φ, v, and S as universal
coordinates of capacity, flow, and entropy.
While this provides coherence, it may oversimplify
human flourishing. Open questions include how to incorporate values like justice, compassion, or
aesthetic meaning into entropic governance, ensuring that slowdown is not only resistant but also
humane.
10

12
Diagnostics and Falsifiable Predictions
The decelerationist agenda makes claims that are not only conceptual but empirically testable.
Each intervention can be coupled to diagnostics that distinguish enrichment effects from symbolic
gestures, ensuring that the framework remains accountable to evidence.
1. Cipher KL bound.
Across personalized glyphs, model decoding error should scale with
KL(pΣi∥pΣ).
A falsification would occur if large language models trained on canonical cor-
pora could generalize across ciphered corpora with negligible error. Open question: to what
extent does human fluency in cipher use outpace machine adaptation, and for how long can
entropy be sustained before convergence?
2. Crumple training effect. Students trained on structured reconstruction should outperform
controls on occlusion/overlap tasks. This would demonstrate that lossy entanglement builds
resilience where current models degrade. Limitation: novelty fatigue or overexposure may blunt
the pedagogical effect. Future direction: vary degrees of compression to calibrate optimal S
levels for constructive challenge.
3. Bio-compute transfer. Exposure to microbial growth dynamics should improve mastery of
exponentials and logistics relative to controls, indicating successful transfer from embodied ob-
servation to abstract reasoning. Limitation: classroom implementation may face bio-safety and
resource constraints. Open question: does stochastic variability in cultures enhance or confuse
learning outcomes? Future studies can quantify effect sizes against conventional instruction.
4. Infra coupling. Regions adopting kelp-based composites should show slower growth in data-
center concrete/steel consumption without measurable loss of educational capacity. A falsifi-
cation would occur if regenerative materials introduced fragility or under-capacity. Limitation:
supply chains may initially be too immature to scale. Future direction: life-cycle analyses com-
paring kelp composites to conventional steel/concrete, including embodied energy and carbon
sequestration metrics.
These predictions allow empirical validation of the agenda's eﬀicacy. They also surface lim-
itations and unresolved questions: whether heterogeneity can be maintained at scale, whether
entropy can be continually renewed without overwhelming learners, and whether local adoption
suﬀices against global acceleration pressures.
Future Directions.
To address these uncertainties, pilot programs should be launched in
diverse contexts, with longitudinal tracking of outcomes. Simulation models of Φ, v, S dynamics
can test scenarios before deployment. International collaborations can explore whether pluralization
strategies must be coordinated globally or whether localized entropy buffers suﬀice. Ultimately,
the diagnostics point toward a broader research program: one that treats slowdown not as a slogan
but as a falsifiable, renewable, and humane mode of governance.
11

Conclusion
The Decelerationist Agenda reframes slowdown as plural progress. By diversifying substrates of
learning and infrastructure, we maintain local solvability while preserving global heterogeneity,
compelling AGI to adapt to humanity's variety rather than the reverse.
This stands in stark
contrast to e/acc's accelerationist imperatives, offering a resilient, entropy-respecting path forward.
The core claim is not that progress should be halted, but that it must be refracted. Instead of a
single accelerating channel—standardized curricula, fungible corpora, extractive infrastructures—
the agenda disperses growth into multiple, partially overlapping routes.
Each intervention is
designed to pluralize scalar capacity (Φ), diversify vector flows (v), and elevate entropy (S) in
bounded, constructive ways. Education becomes a lattice of 21 types rather than a funnel; texts
become personal glyphscapes rather than fungible corpora; computation becomes embodied in mi-
crobial dynamics as well as silicon; and school buildings themselves become regenerative, resisting
industrial monoculture.
This program is not without risks. Equity must be preserved, accessibility ensured, and entropy
governed to avoid collapse into noise. Yet these risks are manageable through oversight, adaptation,
and careful piloting. What emerges is a vision of civilization where resilience is not purchased
through control but cultivated through variety.
In philosophical terms, the decelerationist ethos inverts the logic of accelerationism. Where
e/acc seeks to align humanity with entropy maximization through speed, this framework insists
that entropy's value lies in diversity and irreducibility. The challenge for AGI is no longer how
quickly it can assimilate a monoculture, but how it can navigate a plural ecology where no single
model suﬀices.
In this sense, slowdown is enrichment: a widening of trajectories rather than a narrowing. The
Decelerationist Agenda offers a governance model that is not defensive but generative—embedding
RSVP's dynamics into the structures of learning, computation, and infrastructure to secure a future
that is resilient, plural, and irreducibly human.
A
Mathematical Sketch (RSVP)
A.1
RSVP Fields and Continuity
Let M be the manifold of learning states (students × tasks × contexts). Define RSVP fields:
Φ : M →R≥0,
v : M →TM,
S : M →R≥0.
Here Φ is representational capacity, v the flow field, and S the entropy of interpretation. Densities
ρ evolve under a continuity equation with a source term Γ:
∂tρ + ∇· (Φ v) = Γ(Φ, v, S),
∂Γ
∂S ≤0.
(1)
12

Thus local entropy can be resolved (students mastering material) while global entropy increases
(population-level diversity).
A.2
Cipher Fonts and KL Bounds
For student i, let fi : Σ →Σi be a bijective cipher/handwriting map with decoder f−1
i
known to
teachers. A canonical text x is rendered as xi = fi(x). From an information-theoretic perspective,
each cipher induces a distribution pΣi distinct from the canonical pΣ. For a model M without
exposure to fi, expected decoding loss lower-bounds as
E
[
d(M(fi(x)), x)
]
≥
κ KL(pΣi ∥pΣ),
where d is a divergence (e.g. edit distance, cross-entropy). Population heterogeneity across {fi}
thus scales the diﬀiculty of assimilation by a single AGI parser.
A.3
Crumpled Reconstruction as Entropic Folding
Let u : Ω⊂R2 →R denote a page of notes. Crumpling is a piecewise isometry C : Ω→R3 yielding
a ball B = C(Ω). Overlap multiplicity m(y) and curvature κ define an entropic cost
Scrumple =
∫
B
(m(y) −1) dy + λ
∫
folds
κ2 dσ.
(2)
A reconstruction map R is trained to minimize
J [R] = ∥u −R(˜u)∥2 + µ∥∇R∥2,
where ˜u is the folded signal. This functional shows how students practice lowering local entropy
by reconstructing coherence from lossy folds.
A.4
Biological Computation Dynamics
In yogurt-based computation, microbial biomass X(t) and substrate S(t) obey Monod kinetics:
˙X = µmax
S
KS + S X,
˙S = −
1
YX/S µmax
S
KS + S X.
Batch regimes S ≫KS yield X(t) ∼eµmaxt, allowing analog encoding of exponentials and multi-
plicative structure. This expands Φ by adding a biological substrate, diversifies v through metabolic
flows, and sustains high S through stochastic microstates.
13

A.5
Action Functional
A compact RSVP action can be written
A =
∫
M×[0,T]
[
κ
2 Φ−1∥Φv∥2 + β∥∇S∥2 −λ C[G]
]
dV dt,
(3)
where C[G] measures parser idempotence. Interventions like school diversification, cipher fonts,
crumpled reconstruction, bio-computation, and kelp infrastructures contribute additive hetero-
geneity terms that enlarge the basin of high-entropy yet task-solvable phases.
A.6
Diagnostic Predictions
• Cipher heterogeneity ⇒decoding error scales with KL divergence.
• Crumple training ⇒improved robustness on occluded/overlap tasks.
• Yogurt exposure ⇒transfer gains on exponential/logistic reasoning.
• Material diversification ⇒slower coupling between schooling and AGI-relevant infrastructure
growth.
B
Implementation Sketch for Recursive General Parsers
To operationalize the parsing framework within RSVP, we outline a build plan integrating monadized
interfaces, field coupling, idempotent curricula, and diagnostics. This sketch is implementable in
PyTorch or JAX, treating parsing as field-dynamics over data streams.
B.1
Monadized Interfaces
Let G denote a parser functor, with unit η : X →GX, multiplication µ : G(GX) →GX, and
algebra α : GA →A for some space of representations A. In practice, η is implemented as an
encoder mapping inputs into parser state, µ as a recurrent or hierarchical composition operator,
and α as a readout head. Soft losses enforce approximate satisfaction of the monad laws:
∥µ ◦η −id∥
and
∥µ ◦G(η) −id∥,
ensuring stability of recursive application.
B.2
Field Coupling
RSVP fields map onto compute resources:
• Φ (capacity): number of active channels, neurons, or attention heads.
• v (flow): scheduling or routing of messages between layers/agents.
14

• S (entropy): uncertainty budgets, temperature schedules, dropout/noise injections.
Transport of representational mass between nodes can be optimized via Sinkhorn-based optimal
transport, yielding coupling plans π that balance capacity, flow, and entropy constraints.
B.3
Idempotent Curricula
Training proceeds not only on raw tasks x 7→G(x), but also on self-composition:
G(G(x)) ≈G(x).
Divergence between first- and second-order parses is penalized, creating a curriculum that aligns
the model toward idempotence while preserving expressive variability. This mirrors how human
learners stabilize interpretations by re-parsing narratives or problems.
B.4
Diagnostics
Evaluation should probe for:
1. Idempotence test: measure ∥G2 −G∥across distributions.
2. Entropy-budget coupling: track global S against local S resolution during tasks, ensuring
diversity persists without collapse.
3. Phase transition search: sweep parameters (Φ, ∥v∥, S) to detect transitions between brittle
assimilation regimes and robust pluralist regimes.
These diagnostics provide a falsifiable grounding: recursive general parsers should demonstrate
stable self-application, bounded entropy coupling, and resilience across parameter phases.
B.5
Reference Implementation
Below is a PyTorch skeleton:
# rsvp_parser_monad.py
# PyTorch reference skeleton for "parsing as general intelligence"
# Implements ￿(encode), ￿(merge), ￿(realize), Φ (capacity gate), v (routing),
S (uncertainty)
# with monad losses + RSVP field losses + diagnostics.
import math, random
from dataclasses import dataclass
from typing import Dict, Tuple
import torch
import torch.nn as nn
15

import torch.nn.functional as F
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE = torch.float32
torch.set_default_dtype(DTYPE)
# Utilities
def cosine_distance(a: torch.Tensor, b: torch.Tensor, eps=1e-8) -> torch.
Tensor:
a_norm = a / (a.norm(dim=-1, keepdim=True) + eps)
b_norm = b / (b.norm(dim=-1, keepdim=True) + eps)
return 1.0 - (a_norm * b_norm).sum(dim=-1)
def edit_like_distance(logits_a: torch.Tensor, logits_b: torch.Tensor) ->
torch.Tensor:
pa = logits_a.log_softmax(dim=-1)
pb = logits_b.log_softmax(dim=-1)
return (F.kl_div(pa, pb.exp(), reduction="batchmean") + F.kl_div(pb, pa.
exp(), reduction="batchmean")) / 2
def entropy_from_logits(logits: torch.Tensor) -> torch.Tensor:
p = logits.softmax(dim=-1).clamp_min(1e-8)
H = -(p * p.log()).sum(dim=-1)
return H
def sinkhorn(M, num_iters=20, eps=1e-2):
K = torch.exp(M / eps)
r = torch.ones(M.shape[0], M.shape[1], device=M.device)
c = torch.ones(M.shape[0], M.shape[2], device=M.device)
for _ in range(num_iters):
r = 1.0 / (K @ c.unsqueeze(-1)).squeeze(-1).clamp_min(1e-8)
c = 1.0 / (K.transpose(1,2) @ r.unsqueeze(-1)).squeeze(-1).clamp_min(1
e-8)
P = torch.diag_embed(r) @ K @ torch.diag_embed(c)
P = P / P.sum(dim=(-1,-2), keepdim=True).clamp_min(1e-8)
return P
# Toy task (replace with your domain)
class ToyTask(nn.Module):
def __init__(self, vocab=256, max_len=64):
super().__init__()
self.vocab = vocab
self.max_len = max_len
16

def batch(self, B=32) -> Tuple[torch.Tensor, torch.Tensor]:
T = random.randint(self.max_len//2, self.max_len)
x = torch.randint(0, self.vocab, (B, T), device=DEVICE)
y = x.clone()
noise_mask = (torch.rand_like(y.float()) < 0.05)
y[noise_mask] = torch.randint(0, self.vocab, y.shape, device=DEVICE)[
noise_mask]
return x, y
# Parsers and components
class Encoder(nn.Module):
def __init__(self, vocab=256, d_model=512, n_layers=4, n_heads=8):
super().__init__()
self.emb = nn.Embedding(vocab, d_model)
encoder_layer = nn.TransformerEncoderLayer(d_model , n_heads ,
dim_feedforward=4*d_model , batch_first=True)
self.tr = nn.TransformerEncoder(encoder_layer , num_layers=n_layers)
def forward(self, tokens):
z = self.emb(tokens)
p = self.tr(z)
return p
class Merger(nn.Module):
def __init__(self, d_model=512):
super().__init__()
self.lin = nn.Sequential(
nn.Linear(2*d_model , d_model),
nn.GELU(),
nn.Linear(d_model , d_model)
)
def forward(self, p1, p2):
T = min(p1.shape[1], p2.shape[1])
h = torch.cat([p1[:, :T, :], p2[:, :T, :]], dim=-1)
return self.lin(h)
class Realizer(nn.Module):
def __init__(self, d_model=512, vocab=256):
super().__init__()
self.proj = nn.Linear(d_model , vocab)
def forward(self, p):
return self.proj(p)
17

class CapacityGate(nn.Module):
def __init__(self, d_model=512, budget_ratio=0.5):
super().__init__()
self.score = nn.Sequential(
nn.Linear(d_model , d_model//2),
nn.GELU(),
nn.Linear(d_model//2, 1)
)
self.budget_ratio = budget_ratio
def forward(self, p):
raw = self.score(p).squeeze(-1)
g = torch.sigmoid(raw)
B = int(self.budget_ratio * g.shape[1])
topk_vals , _ = torch.topk(g, k=max(1, B), dim=1)
thresh = topk_vals[:, -1:].detach()
g_soft = torch.sigmoid(10.0 * (g - thresh))
return g_soft
class Router(nn.Module):
def __init__(self, d_model=512):
super().__init__()
self.q = nn.Linear(d_model , d_model)
self.k = nn.Linear(d_model , d_model)
self.scale = d_model ** -0.5
self.temp = nn.Parameter(torch.tensor(1.0))
def forward(self, p):
Q, K = self.q(p), self.k(p)
attn_logits = torch.matmul(Q, K.transpose(1, 2)) * self.scale / self.
temp.clamp_min(1e-3)
P = sinkhorn(attn_logits , num_iters=30, eps=1e-1)
return P, attn_logits
class UncertaintyCommittee(nn.Module):
def __init__(self, d_model=512, vocab=256, heads=4, p_drop=0.2):
super().__init__()
self.heads = nn.ModuleList([nn.Sequential(nn.Dropout(p_drop), nn.
Linear(d_model , vocab)) for _ in range(heads)])
def forward(self, p, samples=4):
logits_samples = []
for _ in range(samples):
18

logits_list = [h(p) for h in self.heads]
logits = torch.stack(logits_list , dim=0).mean(0)
logits_samples.append(logits)
logits_stack = torch.stack(logits_samples , dim=0)
mean_logits = logits_stack.mean(0)
ent = entropy_from_logits(mean_logits)
var = logits_stack.var(dim=0).mean(dim=-1)
S = ent + var
return S, mean_logits
@dataclass
class LossWeights:
unitL: float = 1.0
assoc: float = 0.5
alg: float = 0.5
idem: float = 0.5
transport: float = 0.1
continuity: float = 0.1
smooth: float = 0.05
cap_reg: float = 0.05
class GeneralParser(nn.Module):
def __init__(self, vocab=256, d_model=512):
super().__init__()
self.encoder = Encoder(vocab=vocab, d_model=d_model)
self.merger = Merger(d_model=d_model)
self.realizer = Realizer(d_model=d_model , vocab=vocab)
self.gate = CapacityGate(d_model=d_model , budget_ratio=0.5)
self.router = Router(d_model=d_model)
self.committee = UncertaintyCommittee(d_model=d_model , vocab=vocab,
heads=4, p_drop=0.2)
def encode(self, x_tokens):
return self.encoder(x_tokens)
def merge(self, p1, p2):
return self.merger(p1, p2)
def realize(self, p):
return self.realizer(p)
def fields(self, p):
g = self.gate(p)
P, logits_attn = self.router(p)
19

J = P * g.unsqueeze(1)
S, logits_mean = self.committee(p)
return g, P, J, S, logits_mean , logits_attn
def monad_losses(model: GeneralParser , x, y) -> Dict[str, torch.Tensor]:
p1 = model.encode(x)
p2 = model.encode(p1)
p_merge = model.merge(p1, p2)
y_logits = model.realize(p1)
unitL = F.cross_entropy(y_logits.transpose(1,2), y, ignore_index=-100)
y_logits_from_merge = model.realize(p_merge)
y_logits_from_comp = model.realize(model.encode(y_logits.argmax(dim=-1)))
alg = edit_like_distance(y_logits_from_merge , y_logits_from_comp)
p_ggg = model.encode(p2)
left = model.merge(p2, p_ggg)
right = model.merge(p1, model.merge(p1, p2))
assoc = cosine_distance(left.mean(dim=1), right.mean(dim=1)).mean()
idem = cosine_distance(p2.mean(dim=1), p1.mean(dim=1)).mean()
task = F.cross_entropy(y_logits.transpose(1,2), y, ignore_index=-100)
return dict(task=task, unitL=unitL, alg=alg, assoc=assoc, idem=idem,
p1=p1, p2=p2, y_logits=y_logits)
def rsvp_field_losses(model: GeneralParser , p1, S_weight=1.0) -> Dict[str,
torch.Tensor]:
g, P, J, S, mean_logits , attn_logits = model.fields(p1)
B, T, _ = P.shape
flow_sq = (J ** 2).sum(dim=-1)
transport = (flow_sq / (g + 1e-6)).mean()
incoming = J.sum(dim=1)
outgoing = J.sum(dim=2)
div = incoming - outgoing
continuity = (div ** 2).mean()
grad_S = (S[:, 1:] - S[:, :-1])
smooth = (grad_S ** 2).mean() + 0.1 * S.mean()
cap_reg = (g.mean(dim=1) - 0.5).abs().mean()
return dict(transport=transport , continuity=continuity , smooth=smooth,
cap_reg=cap_reg ,
g=g, P=P, J=J, S=S, mean_logits=mean_logits)
@torch.no_grad()
20

def idempotence_curve(model: GeneralParser , task: ToyTask , k_max=5, B=64):
xs, ys = task.batch(B)
xs, ys = xs.to(DEVICE), ys.to(DEVICE)
p = model.encode(xs)
vals = []
for k in range(2, k_max+1):
p_next = model.encode(p)
d = cosine_distance(p_next.mean(dim=1), p.mean(dim=1)).mean().item()
vals.append((k, d))
p = p_next
return vals
@torch.no_grad()
def entropy_flux_coupling(model: GeneralParser , x):
p = model.encode(x)
g, P, J, S, _, _ = model.fields(p)
incoming = J.sum(dim=1); outgoing = J.sum(dim=2)
div = incoming - outgoing
p_next = model.encode(p)
g2, P2, J2, S2, _, _ = model.fields(p_next)
dS = S2 - S
v1 = torch.stack([g.flatten(), div.flatten()], dim=-1)
v2 = (-dS).flatten()
v1 = (v1 - v1.mean(dim=0)) / (v1.std(dim=0) + 1e-6)
v2 = (v2 - v2.mean()) / (v2.std() + 1e-6)
coef = torch.linalg.lstsq(v1, v2.unsqueeze(-1)).solution.squeeze()
return {"coef_g": coef[0].item(), "coef_divJ": coef[1].item()}
def train(num_steps=2000, vocab=256, d_model=256, batch_size=32, lr=3e-4,
weights=LossWeights()):
task = ToyTask(vocab=vocab, max_len=64)
model = GeneralParser(vocab=vocab, d_model=d_model).to(DEVICE)
opt = torch.optim.AdamW(model.parameters(), lr=lr)
for step in range(1, num_steps+1):
x, y = task.batch(batch_size)
x, y = x.to(DEVICE), y.to(DEVICE)
ml = monad_losses(model, x, y)
fl = rsvp_field_losses(model, ml["p1"])
L = (
ml["task"]
+ weights.unitL * ml["unitL"]
21

+ weights.assoc * ml["assoc"]
+ weights.alg * ml["alg"]
+ weights.idem * ml["idem"]
+ weights.transport * fl["transport"]
+ weights.continuity * fl["continuity"]
+ weights.smooth * fl["smooth"]
+ weights.cap_reg * fl["cap_reg"]
)
opt.zero_grad(set_to_none=True)
L.backward()
torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
opt.step()
if step % 100 == 0:
with torch.no_grad():
idems = idempotence_curve(model, task, k_max=4, B=batch_size)
cpl = entropy_flux_coupling(model, x)
print(f"[{step}]␣"
f"loss={L.item():.3f}␣task={ml['task '].item():.3f}␣idem={ml
['idem'].item():.3f}␣"
f"trans={fl['transport '].item():.3f}␣cont={fl['continuity '].
item():.3f}␣"
f"smooth={fl['smooth '].item():.3f}␣cap={fl['cap_reg '].item()
:.3f}␣"
f"|␣IdemCurve={[(k,␣round(v,3))␣for␣k,v␣in␣idems]}␣"
f"|␣Coupling={{{'g':␣round(cpl['coef_g '],3),␣'divJ':␣round(
cpl['coef_divJ '],3)}}}")
return model
if __name__ == "__main__":
_ = train()
References
[1] Accelerationism. https://en.wikipedia.org/wiki/Accelerationism. Accessed 2025-10-03.
[2] Effective accelerationism. https://en.wikipedia.org/wiki/Effective_accelerationism. Accessed
2025-10-03.
[swa] Effective accelerationism. https://en.wikipedia.org/wiki/Effective_accelerationism. Criti-
cism by David Swan.
22

[4] Instrumental convergence. https://en.wikipedia.org/wiki/Instrumental_convergence. Accessed
2025-10-03.
[5] (2005). Parsing a cognitive task: A characterization of the mind's bottleneck. https://pmc.ncbi.nlm.
nih.gov/articles/PMC546328/.
[6] (2009). Idempotent monads and ⋆-functors. https://arxiv.org/abs/0909.3162.
[7] (2009). Parsing pcfg within a general probabilistic inference framework. https://www.atlantis-press.
com/proceedings/agi09/1869.
[8] (2010). Idempotent monads and ￿-functors. https://www.sciencedirect.com/science/article/pii/
S0022404910000794.
[9] (2010).
Recursive
self-improvement.
https://www.lesswrong.com/tag/
recursive-self-improvement.
[10] (2011). Parsing - traxler - 2011 - wires cognitive science. https://wires.onlinelibrary.wiley.com/
doi/abs/10.1002/wcs.112.
[11] (2017).
First
support
for
a
physics
theory
of
life.
https://www.quantamagazine.org/
first-support-for-a-physics-theory-of-life-20170726/.
[12] (2019).
Not
all
monads
are
idempotent,
a
cautionary
tale
on
natural
...
https://math.stackexchange.com/questions/3407138/
not-all-monads-are-idempotent-a-cautionary-tale-on-natural-transformations.
[13] (2019).
The unavoidable problem of self-improvement in ai.
https://futureoflife.org/ai/
the-unavoidable-problem-of-self-improvement-in-ai-an-interview-with-ramana-kumar-part-1/.
[14] (2020). Cognitive psychology: Parsing part 1. https://www.youtube.com/watch?v=ichL92WrnBA.
[15] (2020).
Psycholinguistics/parsing.
https://en.wikiversity.org/wiki/Psycholinguistics/
Parsing.
[16] (2021). Close the loop of neural perception, grammar parsing, and ... https://escholarship.org/
uc/item/9df1h759.
[17] (2021).
Parsing as a cue‐based retrieval model.
https://pmc.ncbi.nlm.nih.gov/articles/
PMC8459291/.
[18] (2021a). Parsing model and a rational theory of memory. https://www.frontiersin.org/journals/
psychology/articles/10.3389/fpsyg.2021.657705/full.
[19] (2021b). Parsing model and a rational theory of memory. https://www.frontiersin.org/journals/
psychology/articles/10.3389/fpsyg.2021.657705/full.
[20] (2022).
Free idempotent monad associated to a monad.
https://mathoverflow.net/questions/
433736/free-idempotent-monad-associated-to-a-monad.
[21] (2023). Been diving into e/acc or effective accelerationism recently... https://x.com/kaulout/status/
1688966104259956746.
23

[22] (2023).
'effective accelerationism' doesn't care if humans are replaced by ai.
https://www.
businessinsider.com/effective-accelerationism-humans-replaced-by-ai-2023-12.
[23] (2023).
My thoughts and critique on effective accelerationism or e/acc...
https://x.com/
irinimalliaraki/status/1735969074557915558.
[24] (2023).
The real ai fight. effective accellerationists and....
https://doctorow.medium.com/
the-real-ai-fight-1ce751886457.
[25] (2023).
Thoughts on effective accelerationism?
https://www.reddit.com/r/CriticalTheory/
comments/171mbo3/thoughts_on_effective_accelerationism/.
[26] (2023).
What
are
some
good
critiques
of
'e/acc'
('effective
accelera-
tionism')?
https://forum.effectivealtruism.org/posts/YJm3B6d6a3pxTg3KG/
what-are-some-good-critiques-of-e-acc-effective.
[27] (2023). What's the deal with effective accelerationism (e/acc)? https://www.lesswrong.com/posts/
2ss6gomAJdqjwdSCy/what-s-the-deal-with-effective-accelerationism-e-acc.
[28] (2024). Accelerationism. https://www.britannica.com/topic/accelerationism. Accessed 2025-10-
03.
[29] (2024).
Citation needed by molly white.
https://citationneeded.news/.
Critique referenced in
tweet.
[30] (2024). A critique of both effective altruism and effective accelerationism... https://x.com/mbauwens/
status/1743165414849597551.
[31] (2024). Paper shows gpt gains general intelligence from data: Path to agi. https://www.reddit.com/
r/OpenAI/comments/1g1q8e6/paper_shows_gpt_gains_general_intelligence_from/.
[32] (2024). A quick q&a on the 'effective accelerationism' (e/acc) movement. https://fasterplease.
substack.com/p/a-quick-q-and-a-on-the-effective.
[33] (2024). A scathing critique of accelerationism: E/acc, d/acc, ea, and punk. https://www.linkedin.
com/pulse/scathing-critique-accelerationism-eacc-dacc-ea-punk-ref-gavriel-shaw-6lete.
[34] (2024). What does artificial general intelligence actually mean? https://www.scientificamerican.
com/article/what-does-artificial-general-intelligence-actually-mean/.
[35] (2024). What is artificial general intelligence (agi)? https://www.mckinsey.com/featured-insights/
mckinsey-explainers/what-is-artificial-general-intelligence-agi.
[36] (2024). What is meant by agi? on the definition of artificial general ... https://arxiv.org/html/
2404.10731v1.
[37] (2025).
Artificial general intelligence.
https://en.wikipedia.org/wiki/Artificial_general_
intelligence.
[38] (2025).
Artificial
general
intelligence.
https://nmu.edu/ai-literacy-initiative/
general-intelligence-ai.
24

[39] (2025a). General intelligence requires reward-based pretraining. https://arxiv.org/abs/2502.19402.
[40] (2025b). Idempotence for relative monads. https://arxiv.org/abs/2508.17794.
[41] (2025). idempotent monad in nlab. https://ncatlab.org/nlab/show/idempotent%2Bmonad.
[42] (2025).
Parsing ai. the term "artificial intelligence" (ai)....
https://johnjparman.medium.com/
parsing-ai-4c988565e776.
[43] (2025). The reality of recursive improvement: How ai automates its own ... https://aiprospects.
substack.com/p/the-reality-of-recursive-improvement.
[44] (2025). Recursive self-improvement. https://www.lesswrong.com/w/recursive-self-improvement.
[45] (2025).
The self-evolving machine: Recursive self-improvement in agi.
https://saidar.ai/blog/
the-self-evolving-machine.
[46] (2025). What is accelerationism? a primer on the defining philosophy of the 2020s. https://www.
realitystudies.co/p/what-is-accelerationism-effective-eacc-nick-land-mark-fisher.
[47] (2025). What is agi? - artificial general intelligence explained. https://aws.amazon.com/what-is/
artificial-general-intelligence/.
[48] (2025).
What is artificial general intelligence (agi)?
https://www.ibm.com/think/topics/
artificial-general-intelligence.
[49] (2025). What is artificial general intelligence? definition and examples. https://www.coursera.org/
articles/what-is-artificial-general-intelligence.
[50] (2025).
What is parsing in cognitive psychology?
https://homework.study.com/explanation/
what-is-parsing-in-cognitive-psychology.html.
[51] (2025). Why will we not achieve recursive self-improvement within a year? https://www.reddit.com/
r/singularity/comments/1i89id3/why_will_we_not_achieve_recursive_selfimprovement/.
[52] Andreessen,
M.
(2023).
The
techno-optimist
manifesto.
https://a16z.com/
the-techno-optimist-manifesto/.
[53] Bostrom, N. (2012). The superintelligent will: Motivation and instrumental rationality in advanced
artificial agents.
[54] England, J. L. (2015).
Dissipative adaptation in driven self-assembly.
Nature Nanotechnology,
10(11):919-923.
[55] Fried, I. and Heath, R. (2023). Civilization depends on more ai, marc andreessen says. https://www.
axios.com/2023/10/17/marc-andreessen-ai-manifesto-techno-optimist.
[56] Horowitz, J. M. et al. (2017). Spontaneous fine-tuning to environment in many-species chemical reaction
networks. Proceedings of the National Academy of Sciences, 114(28):7565-7570.
[57] Nolan, B. (2023).
The 'effective accelerationism' movement doesn't care if humans are replaced
by ai as long as they're there to make money from it.
https://www.businessinsider.com/
effective-accelerationism-humans-replaced-by-ai-2023-12.
25

[58] Omohundro, S. M. (2008). The basic ai drives. In Proceedings of the 2008 Conference on Artificial
General Intelligence.
[59] Wolchover, N. (2017). First support for a physics theory of life. https://www.quantamagazine.org/
first-support-for-a-physics-theory-of-life-20170726/.
[60] Zamora, K., Jarenwattananon, P., and Shapiro, A. (2024).
Secretary of commerce gina rai-
mondo says "congress needs to act" on ai regulation.
https://www.ideastream.org/2024-05-22/
secretary-of-commerce-gina-raimondo-says-congress-needs-to-act-on-ai-regulation.
26

